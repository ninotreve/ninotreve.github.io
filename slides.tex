\documentclass{beamer}
\usetheme{Hannover}
\usepackage{amssymb,amsmath}
\usepackage{setspace} 
\usepackage{mathrsfs}
\setbeamertemplate{caption}[numbered]

\title{The Slice Sampler}
\author{Yuejia Zhang}
\begin{document}
	\begin{frame}[plain]
		\maketitle
	\end{frame}
\section{Introduction}
	\begin{frame}{Introduction}
		While many of the MCMC algorithms presented in the previous chapter are
		both generic and universal, there exists a special class of MCMC
		algorithms that are more model dependent in that they exploit 
		the local conditional features of the distributions to simulate.
		
		Metropolis--Hastings algorithms can achieve higher levels of efficiency when
		they take into account the specifics of the target distribution $f$, in particular
		through the calibration of the acceptance rate.
	\end{frame}
\section{2D Slice Sampler}
	\begin{frame}{The Fundamental Theorem}
		The generation from a distribution with density
		$f(x)$ is equivalent to uniform generation on the subgraph of $f$,
		$$
		\mathscr{S}(f)=\{(x, u) ; 0 \leq u \leq f(x)\},
		$$
		whatever the dimension of $x$, and $f$ need only be known up to a normalizing constant.
		
		\vspace{0.5cm}
		
		Since $(X, U) \sim \mathcal{U}(\{(x, u) ; 0 \leq u \leq f(x)\})$,
		the marginal density $\int_{\mathscr{S}} f(x, u) \, \mathrm{d} u = \int_0^{f(x)} \, \mathrm{d} u = f(x)$.
	\end{frame}

	\begin{frame}{2D Slice Sampler}
		Another solution is to use a random walk on $\mathscr{S}(f)$. 
		There are many ways of implementing a random walk on this set, but a
		natural solution is to go one direction at a time.
		
		Starting from a point $(x, u)$ in $\{(x, u): 0<u<f(x)\}$, the move along the $u$-axis will correspond to the conditional distribution
		$$
		U \mid X=x \sim \mathcal{U}(\{u: u \leq f(x)\}),
		$$
		resulting in a change from point $(x, u)$ to point $\left(x, u^{\prime}\right)$, still in $\mathscr{S}(f)$, and then the move along the $x$-axis to the conditional distribution
		$$
		X \mid U=u^{\prime} \sim \mathcal{U}\left(\left\{x: u^{\prime} \leq f(x)\right\}\right),
		$$
		resulting in a change from point $\left(x, u^{\prime}\right)$ to point $\left(x^{\prime}, u^{\prime}\right)$.

	\end{frame}
	\begin{frame}{2D Slice Sampler}
		\begin{block}{Algorithm: 2D Slice Sampler}
	\texttt{At iteration $t$, simulate}
	\begin{enumerate}
		\item $u^{(t+1)} \sim \mathcal{U}_{\left[0, f(x^{(t)})\right]}$;
		\item $x^{(t+1)} \sim \mathcal{U}_{A^{(t+1)}}$, with
		$$
		A^{(t+1)}=\left\{x: f(x) \geq u^{(t+1)}\right\}.
		$$
	\end{enumerate}
\end{block}

It's easy to see that $x^{(t)}$ is always part of the set $A^{(t+1)}$, which is thus nonempty. Moreover, the algorithm remains valid if $f(x)=C \cdot f_{1}(x)$, and we use $f_{1}$ instead of $f$. This is quite advantageous in settings where $f$ is an unnormalized density like a posterior density.
	\end{frame}
	\begin{frame}{Why It Works} \small
We now verify that the uniform distribution on $\mathscr{S}$ is indeed a stationary distribution of the chain. Suppose $(x, u)$'s distribution is uniform on $\mathscr{S}$ and the random walk on $\mathscr{S}$ generates sequence $(x, u) \rightarrow(x, v) \rightarrow$ $(y, v)$, then we have
$$
\begin{aligned}
	P\bigl((y, v)\bigr) &=\iint_{\mathbb{R}^2} p\bigl((x, u)\bigr) \cdot p \bigl((x, v)|(x, u)\bigr) \cdot p \bigl((y, v)|(x, v)\bigl) \, \text{d}u\text{d}x \\
	&=\iint_{\mathbb{R}^2} \mathbb{I}_{0 \leq u \leq f(x)} \cdot \frac{\mathbb{I}_{0 \leq v \leq f(x)}(v)}{f(x)} \cdot \frac{\mathbb{I}_{\{y \mid f(y) \geq v\}}(y)}{\operatorname{mes}(\{z|f(z) \geq v\})} \, \text{d}u\text{d}x \\
	&=\int_{\mathbb{R}} \mathbb{I}_{0 \leq v \leq f(x)}(v) \cdot \frac{\mathbb{I}_{\{y \mid f(y) \geq v\}}(y)}{\operatorname{mes}(\{z|f(z) \geq v\})} \,\text{d}x \\
	&=\mathbb{I}_{0 \leq v \leq f(y)}
\end{aligned}
$$
where $\operatorname{mes}(A)$ denotes the (generally Lebesgue) measure of the set $A$.
	\end{frame}

\begin{frame}{Examples}
	\begin{example}[Simple slice sampler]
		Consider the density $f(x)=\frac{1}{2} e^{-\sqrt{x}}$ for $x>0$. While it can be directly simulated from, it also yields easily to the slice sampler. We have
		$$
		U \mid x \sim \mathcal{U}\left(0, \frac{1}{2} e^{-\sqrt{x}}\right), \quad X \mid u \sim \mathcal{U}\left(0,[\log (2 u)]^{2}\right).
		$$
		We implement the sampler to generate $50,000$ variates, and plot them along with the density in Figure 1, which shows that the agreement is very good. The performances of the slice sampler may however deteriorate when $\sqrt{x}$ is replaced with $x^{1 / d}$ for $d$ large enough.
	\end{example}
\end{frame}
\begin{frame}{Examples}
	\begin{figure}[htbp]
		\includegraphics[scale=0.7]{fig/1.png}
		\caption{Simple slice sampler histogram and density}
	\end{figure}
\end{frame}
\begin{frame}{Examples}
	\begin{example}[Truncated normal distribution]
	The distribution represented in Figure 2 is a truncated normal distribution $\mathcal{N}_{-}^{+}(3,1)$, restricted to the interval $[0,1]$,
	$$
	f(x) \propto f_{1}(x)=\exp \bigl(-(x+3)^{2} / 2\bigr) \, \mathbb{I}_{[0,1]}(x).
	$$
	The na√Øve simulation of a normal $\mathcal{N}(3,1)$ random variable until the outcome is in $[0,1]$ is suboptimal because there is only a $2 \%$ probability of this happening. We may use the Accept--Reject algorithm where the instrumental density is $g_\alpha \propto \frac{1}{\alpha} \exp \bigl(-\alpha x \bigr)$. Compute the ratio $f/g_\alpha$ and solve the optimal $\alpha$.
	\end{example}
\end{frame}
\begin{frame}{Examples}
	\begin{example}[Truncated normal distribution]
	The slice sampler applied to this problem is then associated with the horizontal slice
	$$
	A^{(t+1)}=\left\{y \in [0,1] : \exp \bigl(-(y+3)^{2} / 2\bigr) \geq u \right\}. 
	$$
	Figure 2 shows the first ten steps of slice sampler started from $x^{(0)}=0.25$. Figure 3 illustrates the very limited dependence on the starting value for the truncated normal example.
\end{example}
\end{frame}
\begin{frame}{Examples}
	\begin{figure}[htbp]
		\includegraphics[scale=0.6]{fig/2.png}
		\caption{First ten steps of the slice sampler for the truncated normal distribution $\mathcal{N}_{-}^{+}(3, 1)$, restricted to the interval $[0, 1]$.}
	\end{figure}
\end{frame}
	\begin{frame}{Examples}
	\begin{figure}[htbp]
			\includegraphics[scale=0.4]{fig/3.png}
			\caption{Comparison of three samples obtained by ten iterations of the slice sampler starting from $(.01, .01)$ (left), $(.99, .001)$ (center) and $(.25, .025)$ (right) and based on the same pool of uniform variates.}
	\end{figure}
	\end{frame}
	\section{General Slice Sampler}
\begin{frame}{Introduction}
	Sampling uniformly from the slice $A^{(t)}=\left\{x : f_{1}(x) \geq \omega^{(t)}\right\}$ may be completely intractable. This difficulty persists as the dimension of $x$ gets larger. 
	
	However, there exists a generalization of the 2D slice sampler that partially alleviates this difficulty by introducing multiple slices. It relies upon the decomposition of the density $f(x)$ as
	$$
	f(x) \propto \prod_{i=1}^{k} f_{i}(x)
	$$
	where the $f_{i}$ 's are positive functions, but not necessarily densities. For instance, in a Bayesian framework with a flat prior, the $f_{i}(x)$ may be chosen as the individual likelihoods.
\end{frame}
\begin{frame}{General Slice Sampler}
	\begin{block}{Algorithm: General Slice Sampler}
		\begin{spacing}{1.5}
	\texttt{At iteration $t+1$, simulate}

	\texttt{1.} $\omega_{1}^{(t+1)} \sim \mathcal{U}_{[0, f_{1}(x^{(t)})]}$;
	
	$\vdots$
	
	\texttt{k.} $\omega_{k}^{(t+1)} \sim \mathcal{U}_{[0, f_{k}(x^{(t)})]}$;
	
	\texttt{k+1.} $x^{(t+1)} \sim \mathcal{U}_{A(t+1)}$, \texttt{with}

$$
A^{(t+1)}=\left\{y : f_{i}(y) \geq \omega_{i}^{(t+1)}, i=1, \ldots, k\right\}
$$
\end{spacing}
	\end{block}
\end{frame}
\begin{frame}{Examples}
	\begin{example}[A 3D slice sampler]
	Consider the density proportional to
$$
\left(1+\sin ^{2}(3 x)\right)\left(1+\cos ^{4}(5 x)\right) \exp \left\{-x^{2} / 2\right\} .
$$
In an iteration of the slice sampler, three uniform $\mathcal{U}([0,1]) u_{1}, u_{2}, u_{3}$ are generated and the new value of $x$ is uniformly distributed over the set
$$
\begin{aligned}
\left\{x:|x| \leq \sqrt{-2 \log \omega_{3}}\right\} &\cap\left\{x: \sin ^{2}(3 x) \geq 1-\omega_{1}\right\} \\ &\cap\left\{x: \cos ^{4}(5 x) \geq 1-\omega_{2}\right\},
\end{aligned}
$$
which is made of one or several intervals depending on whether $\omega_{1}=u_{1} f_{1}(x)$ and $\omega_{2}=u_{2} f_{2}(x)$ are larger than 1. 
	\end{example}

\end{frame}
\begin{frame}{Examples}
	\begin{figure}[htbp]
		\includegraphics[scale=0.5]{fig/4.png}
		\caption{Histogram of a sample produced by $5,000$ iterations of a 3D slice sampler and superposition of the target density proportional to $f$.}
	\end{figure}
\end{frame}

\section{Connection with Gibbs Sampler}
\begin{frame}{\large From Slice Sampling to Gibbs Sampling} 
	\small
	Instead of a density $f_{X}(x)$, consider now a joint density $f(x, y)$ defined on an arbitrary product space, $\mathscr{X} \times \mathscr{Y} .$ If we use the fundamental theorem of simulation in this setup, we simulate a uniform distribution on the set
	$$
	\mathscr{S}(f)=\{(x, y, u): 0 \leq u \leq f(x, y)\}
	$$
	Starting at a point $(x, y, u)$ in $\mathscr{S}(f)$, we generate
	\begin{enumerate}
	\item $X$ along the $x$-axis from the uniform distribution on $\{x: u \leq f(x, y)\}$,
\item $Y$ along the $y$-axis from the uniform distribution on $\left\{y: u \leq f\left(x^{\prime}, y\right)\right\}$,
\item $U$ along the $u$-axis from the uniform distribution on $\left[0, f\left(x^{\prime}, y^{\prime}\right)\right]$.
	\end{enumerate}

\end{frame}
\begin{frame}{\large From Slice Sampling to Gibbs Sampling}
		\small
	Note that generating from the uniform distribution on $\{x: u \leq f(x, y)\}$ is equivalent to generating from the uniform distribution
	$$
	\left\{x: f_{X \mid Y}(x \mid y) \geq u / f_{Y}(y)\right\}.
	$$
	Also, the sequence of uniform generations along the three axes does not need to be done in the same order $x-y-u$ all the time for the Markov chain to remain stationary with stationary distribution the uniform on $\mathscr{S}(f)$.
	
	Consider the limiting case where the $X$ and the $U$ simulations are repeated an infinite number of times before moving to the $Y$ simulation, we end up with a simulation (in $X$ ) of $$
	X \sim f_{X \mid Y}(x \mid y).
	$$
	Consider now the same repetition of $Y$ and $U$ simulations with $X$ fixed at its latest value: in the limiting case, this produces a simulation (in $Y$ ) of
	$$
	Y \sim f_{Y \mid X}(y \mid x) .
	$$
\end{frame}
\begin{frame}{Back to the Slice Sampler}
The slice sampler starts with $f_{X}(x)$ and creates a joint density $f(x, u)=\mathbb{I}(0<u<$ $\left.f_{X}(x)\right)$. The associated conditional densities are
$$
f_{X \mid U}(x|u)=\frac{\mathbb{I}_{0<u<f_{X}(x)}}{\int \mathbb{I}_{0<u<f_{X}(x)} d x} 
\text{ and } 
f_{U \mid X}(u|x)=\frac{\mathbb{I}_{0<u<f_{X}(x)}}{\int \mathbb{I}_{0<u<f_{X}(x)} d u}
$$ 
which are exactly those used in the slice sampler. Therefore, the $X$ sequence is also a Markov chain with transition kernel
$$
K\left(x, x^{\prime}\right)=\int f_{X \mid U}\left(x^{\prime}| u\right) f_{U \mid X}(u|x) d u
$$
and stationary density $f_{X}(x)$.
\end{frame}

\begin{frame}{Summary}
\begin{enumerate}
	\item The two-stage Gibbs sampler can be seen as a limiting case of the slice sampler in a three-coordinate problem.
	\item The slice sampler can be interpreted as a special case of two-stage Gibbs sampler when the joint distribution is the uniform distribution on the subgraph $\mathscr{S}(f)$.
	\item We can induce a Gibbs sampler for any marginal distribution $f_{X}(x)$ by creating a joint distribution that is, formally, arbitrary. Starting from $f_{X}(x)$, we can take any conditional density $g(y|x)$ and create a Gibbs sampler with
	$$
	f_{X \mid Y}(x|y)=\frac{g(y|x) f_{X}(x)}{\int g(y|x) f_{X}(x) d x}
	$$
	and
	$$
	f_{Y \mid X}(y|x)=\frac{g(y|x) f_{X}(x)}{\int g(y|x) f_{X}(x) d y}.
	$$
\end{enumerate}
\end{frame}
\begin{frame}{Examples}
	\begin{example}[Censored data models]
		Censored data models can be associated with a missing data structure. Consider
		$$
		y^{*}=\left(y_{1}^{*}, y_{2}^{*}\right)=\left(y \wedge r, \mathbb{I}_{y<r}\right), \quad \text { with } y \sim f(y|\theta) \text { and } r \sim h(r)
		$$
		so the observation $y$ is censored by the random variable $r$. If we observe $\left(y_{1}^{*}, \ldots, y_{n}^{*}\right)$, the density of $y_{i}^{*}=\left(y_{1 i}^{*}, y_{2 i}^{*}\right)$ is then
		$$
		\int_{y_{1 i}^{*}}^{+\infty} f(y \mid \theta) d y h\left(y_{1 i}^{*}\right) \mathbb{I}_{y_{2 i}^{*}=0}+\int_{y_{1 i}^{*}}^{+\infty} h(r) d r f\left(y_{1 i}^{*} \mid \theta\right) \mathbb{I}_{y_{2 i}^{*}=1}
		$$
		The likelihood and
		posterior distribution associated with this model may be too complex to be
		used.
	\end{example}
\end{frame}

\begin{frame}{Examples}
	\small
	Recall Gibbs Sampling:
	Recall the censored-data model: There are $n$ samples $X=\left(x_{1}, \ldots, x_{m}, z_{m+1}, \ldots, z_{n}\right)$, each corresponding to the density $f(x-\theta)$ (Here it is $\mathcal{N}(\theta, 1)$ ), with the first $m$ fully observed and the last $n-m$ censored at $a$. Then we can identify $g(x \mid \theta)$ with the likelihood function:
	$$
	g(x \mid \theta)=L(\theta \mid x) \propto \prod_{i=1}^{m} \mathrm{e}^{-\left(x_{i}-\theta\right)^{2} / 2},
	$$
	and
	$$
	f(x, z \mid \theta)=L(\theta \mid x, z) \propto \prod_{i=1}^{m} \mathrm{e}^{-\left(x_{i}-\theta\right)^{2} / 2} \prod_{i=m+1}^{n} \mathrm{e}^{-\left(z_{i}-\theta\right)^{2} / 2}
	$$
	Given a prior distribution $\pi(\theta)$ on $\theta$, we can obtain $\pi(\theta \mid x, z)$ and $f(z \mid x, \theta)$. Using a Gibbs sampler based on these two conditionals, we can simulate the posterior distribution of $(\theta, Z)$ with a Gibbs sampler.
\end{frame}

\begin{frame}{Examples} \small
	\begin{example}[Censored data models]
		In Gibbs sampling, a logical completion is to reconstruct the original data, $y_{1}, \ldots, y_{n}$, conditionally on the observed $y_{i}^{*}$ 's $(i=1, \ldots, n)$, and then implement the Gibbs sampler on the two groups $\theta$ and the unobserved $y$ 's. Here we can push the demarginalization further to first represent the above posterior as the marginal of
		$$
		\pi(\theta) \prod_{\left\{i: y_{2 i}^{*}=0\right\}} f\left(y_{1 i} \mid \theta\right) \mathbb{I}_{y_{1 i}>y_{1 i}^{*}} \prod_{\left\{i: y_{2 i}^{*}=1\right\}} f\left(y_{1 i}^{*} \mid \theta\right)
		$$
		and write this distribution itself as the marginal of
		$$
		\mathbb{I}_{0 \leq \omega_{0} \leq \pi(\theta)} \prod_{\left\{i: y_{2 i}^{*}=0\right\}}\left\{\mathbb{I}_{y_{1 i}>y_{1 i}^{*}} \mathbb{I}_{0 \leq \omega_{i} \leq f\left(y_{1 i} \mid \theta\right)}\right\} \prod_{\left\{i: y_{2 i}^{*}=1\right\}} \mathbb{I}_{0 \leq \omega_{i} \leq f\left(y_{1 i}^{*} \mid \theta\right)}
		$$
		adding to the unobserved $y_{1 i}$ 's the basic auxiliary variables $\omega_{i}(0 \leq i \leq n) .$
	\end{example}
	
\end{frame}
\section{Convergence Properties of the Slice Samplers}
\begin{frame}{Stationary and Ergodicity}
	MCMC proceeds by constructing a Markov chain $\mathcal{X}$, with transition probabilities $P(x, \cdot)$, such that $\pi(\cdot)$ is a stationary distribution for this chain, i.e.,
	$$
	\pi P(\cdot) \equiv \int \pi(d x) P(x, \cdot)=\pi(\cdot)
	$$
	One then hopes that, if this chain is simulated long enough on a computer, with resulting values $X_{0}, X_{1}, \ldots$, the distributions $\mathcal{L}\left(X_{n}\right)$ will eventually be approximately $\pi(\cdot)$.
	
	However, just because $\pi(\cdot)$ is a stationary distribution for $P(x, \cdot)$, it does not follow that the distributions $\mathcal{L}\left(X_{n}\right)$ will necessarily converge to $\pi(\cdot)$, as $n \rightarrow \infty$. Ergodicity is used to characterized that the stationary distribution is also a limiting distribution in the sense that the limiting distribution of $X_n$ is $f$ for almost any initial value $X_0$.
\end{frame}

\begin{frame}{Ergodicity}
	If the chain is $\phi$-irreducible and aperiodic, then it follows that we will have asymptotic convergence in total variation distance from almost every starting point. Here " $\phi$-irreducible" means that there is some non-zero measure $\phi$ (e.g. Lebesgue measure) such that, for every set $A$ with $\phi(A)>0$, there is positive probability that the chain will eventually enter the set $A$ started from any starting value $x \in \mathcal{X}$. Also "aperiodic" means that $\mathcal{X}$ does not contain nonempty disjoint subsets $\mathcal{X}_{1}, \mathcal{X}_{2}, \ldots \mathcal{X}_{j}$, with $j \geq 2$, such that $P\left(x, \mathcal{X}_{i+1 \bmod j}\right)=1$ whenever $x \in \mathcal{X}_{i}$. Finally,
	the total variation distance between two probability measures $\mu$ and $\nu$ is defined to be $\|\mu-\nu\| \equiv \sup _{A \subseteq \mathcal{X}}|\mu(A)-\nu(A)|$. 
\end{frame}
\begin{frame}{Ergodicity}
In terms of these definitions, the formal statement of the convergence theorem is as follows: Let $P(x, \cdot)$ be the transition probabilities for a Markov chain on a state space $\mathcal{X}$, with stationary distribution $\pi(\cdot)$. Suppose the chain is $\phi$-irreducible and aperiodic. Then for $\pi$-almost all starting points $x \in \mathcal{X}$, we have that as $n \rightarrow \infty$,
$$
\left\|\mathcal{L}\left(X_{n} \mid X_{0}=x\right)-\pi(\cdot)\right\| \rightarrow 0
$$
A chain is \emph{geometrically ergodic} if for $\pi$-almost all $x \in \mathcal{X}$, there is $\rho<1$ and $M(x)<\infty$, such that $\left\|\mathcal{L}\left(X_{n} \mid X_{0}=x\right)-\pi(\cdot)\right\| \leq M(x) \rho^{n}$. A chain is \emph{uniformly ergodic} if it is geometrically ergodic and the decreasing rate is uniform over the whole space.
\end{frame}
\begin{frame}{Main Convergence Result}
	\begin{block}{Lemma}
If $f_{1}$ is bounded and $\operatorname{supp} f_{1}$ is bounded, the slice sampler is uniformly ergodic.
	\end{block}
For the proof, please refer to Roberts and Rosenthal (1999).

\vspace{1cm}
[1] Roberts, G.\,O.\,, and  J.\,S.\,Rosenthal. ``Convergence of Slice Sampler Markov Chains.'' Journal of the Royal Statistical Society: Series B (Statistical Methodology) (1999).
\end{frame}
\end{document}
